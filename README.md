Overview:

This repository contains the work completed during an internship program, focusing on data analysis and machine learning tasks using Python. The project is organized into two levels (I've selected Level 1 and Level 3), each consisting of three key tasks that span across various domains of data science.

Levels:
1. Level 1:
   - Task 1: Data Exploration and Preprocessing
   - Task 2: Descriptive Analysis
   - Task 3: Geospatial Analysis

2. Level 3:
   - Task 1: Predictive Modeling
   - Task 2: Customer Preference Analysis
   - Task 3: Data Visualization

Technologies Used:

- Python (3.x)
- Spyder IDE (for development)
- Pandas, NumPy (for data manipulation)
- Matplotlib, Seaborn (for visualization)
- Scikit-learn (for machine learning)
- Folium (for interactive maps)

Tasks Breakdown:

Level 1:
1. Data Exploration and Preprocessing
   - Understanding and cleaning the dataset.
   - Handling missing values, duplicates, and encoding categorical variables.
   - Performing data type conversion where necessary.

2.Descriptive Analysis
   - Analyzing data to get summary statistics.
   - Using statistical measures (mean, median, mode, standard deviation) to summarize datasets.

3.Geospatial Analysis
   - Analyzing data that includes geographical coordinates (latitude, longitude).
   - Visualizing data points on maps and understanding spatial relationships using library called Folium.
- Visualizing distributions, correlations, and relationships between key features.

Level 2:
1.Predictive Modeling
   - Building a regression model to predict a dependent variable based on
available features.
   - Splitting the dataset into training and testing sets and evaluating the model's performance using appropriate metrics.
- Building and training machine learning models (e.g., linear regression, decision trees, random forest).

2. Customer Preference Analysis
   - Analyzing customer data to determine trends and preferences.
   - Using association techniques to uncover insights into customer behavior.

3.Data Visualization
   - Creating interactive visualizations to represent data insights.
   - Using libraries like Matplotlib, Seaborn, and Plotly to build effective charts (e.g., bar charts, heatmaps, scatter plots, etc.).
 
Installation:

NB:To run the notebooks or scripts in Spyder, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/project-name.git
   ```

2. Navigate to the project directory:
   ```bash
   cd project-name
   ```

3. Create a virtual environment (optional, but recommended):
   ```bash
   python -m venv env
   ```

4. Activate the virtual environment:
   - On Windows:
     ```bash
     .\env\Scripts\activate
     ```
   - On macOS/Linux:
     ```bash
     source env/bin/activate
     ```

5. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

6. Open the project in Spyder:
   - Launch Spyder IDE and open the project directory, then you can start working on the scripts.

Usage

1.Using Spyder:
   - Open the project folder in Spyder by navigating to `File` â†’ `Open Project` and selecting the directory where the project is located.
   - You can then open and run individual Python scripts or Jupyter notebooks directly in Spyder's editor.

2.Run the Python scripts:
   - You can run each task's corresponding script (e.g., `task1_data_exploration.py`) in the Spyder console.

Contacts

For any questions or feedback, feel free to contact me:

- Email: hlongwanehappinesstebogo@gmail.com 
- GitHub: https://github.com/Happiness18?tab=overview&from=2025-02-01&to=2025-02-28
- LinkedIn: https://www.linkedin.com/in/happiness-tebogo-hlongwane-b789a82ba?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app 

Acknowledgments

- Libraries: Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, GeoPandas, etc.
- Mentors and instructors for their guidance throughout the internship.

